{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `４章のテーマ`\n",
    "- 訓練データから最適な重みを自動で設定できるようにする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 データから学習\n",
    "- 機械学習では、データ(今回はMNST画像)から特徴量を抽出して、その特徴量のパターンを学習(規則性)する。\n",
    "\n",
    "#### `特徴量とは`\n",
    "- 入力データから本質的なデータを的確に抽出できるように設計された変換器を指す。\n",
    "- 特徴量はベクトルとして記述される。\n",
    "- **ニューラルネットワークはデータに含まれる重要な特徴量までも「機械」が学習する。**\n",
    "- 例) 5という数字があったら、左上が角ばってて、下が逆c型になっていて、というような特徴量を機会が学習\n",
    "\n",
    "### 4.1.2 訓練データとテストデータ\n",
    "1. 機械学習では訓練(教師)データとテストデータを分ける。\n",
    "2. まず、訓練データで最適なパラメーターを探してモデルを作る\n",
    "3. テストデータでモデルの実力を評価する。\n",
    "\n",
    "↑で評価するのは`汎化能力`(どんなデータにも対応できる)。逆にあるデータに過度に適応した状態を`過学習`と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 損失関数\n",
    "- 分析結果を定量的に表すための関数\n",
    "- 損失関数２条和誤差、交差エントロピー誤差（両方共、大きいほど悪い）\n",
    "\n",
    "###`２乗和誤差`\n",
    "$$ E = 1/2\\sum_{k}(y_k - t_k)^2 $$\n",
    "で表せられる関数でモデル値(yとy2)と教師データ(t)がどれくらい違うかを示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "mean_squared_error(np.array(y), np.array(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y2), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tのように０と１だけの表記方法をone-hot表現という。上記の例では、y2のほうが悪いモデルということになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `交差エントロピー誤差`\n",
    "$$E = -\\sum_{k}(t_k\\log_e Y_k) $$\n",
    "で表せられる関数で、自然対数(log)を使う。この自然対数によってYの部分が１より０に近ければ近いほど、悪いということになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return  -np.sum(t * np.log(y + delta))\n",
    "cross_entropy_error(np.array(y), np.array(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y2), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ミニバッチ学習`\n",
    "$$E = - 1/N \\sum_n \\sum_k t_{nk} \\log Y_{nk}$$\n",
    "の数式では今までの例とは異なり、複数の損失関数の和を求めることができる。\n",
    "そうなってくるとあまりにも大量のデータは処理しきれなくなって来る。\n",
    "\n",
    "そこでランダムにいくつかのデータを引っ張ってきてそれを訓練用データとして採用する方法がある。それがミニバッチ学習である。\n",
    "大量のデータからランダム（今回は１０個）に抜き出すためのコードは下記のものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([41481, 46703, 14504, 16027, 18706, 39980,  9187, 42754, 41898,\n",
       "       35384])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as no\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape) #(60000, 784)\n",
    "print(t_train.shape) #(60000, 10)\n",
    "\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "np.random.choice(60000,10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### `交差エントロピー誤差の実装`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_eroor(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) /batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tは教師データ。データ一つあたり（ｙの次元が１のとき）の交差エントロピー誤差を求めるときにはデータの形状を整形する。最後にバッチの枚数で正規化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下の場合はone-hotの表示形式ではなく、単純に正解（例えば「２」とか「７」とか）をラベルとして与えられたときの実装.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_eroor(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.resshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_saze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上２つの差は最後の行のみ。\n",
    "(t * np.log(y + 1e-7))→(np.log(y[np.arange(batch_size), t] + 1e-7))となっている。\n",
    "後者の、(np.log(y[np.arange(batch_size), t] + 1e-7))の\n",
    "**np.arange(batch_size)**\n",
    "は０からbatch_size-1 までの配列を生成する。（[0, 1, 2, 3, 4, 5.....,batch_size-1])\n",
    "\n",
    "### `ニューラルネットワークで認識精度を指標にしてはいけない`\n",
    "なぜならば、認識精度は最大値を求めにくい（微分したときに０になる場所が多い：つまり変化が少ない）から。一方損失関数は、変動が多く微分を用いて今日k地を見つけやすい。※ちなみにステップ関数も同じ理由で使いにくい。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
